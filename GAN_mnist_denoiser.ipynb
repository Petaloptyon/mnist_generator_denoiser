{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoAR95PcLgce"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKEF1FWcK7kB"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import io\n",
    "import imageio\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import*\n",
    "from keras.models import Sequential, Model\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.applications import VGG19\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bEjm4Y9LlAp"
   },
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4rJVwtu7zXG"
   },
   "source": [
    "## Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARr7_FfBI7No"
   },
   "outputs": [],
   "source": [
    "# function which will add noise for all images in dataset\n",
    "def add_noise(dataset, max_disp):\n",
    "    noise = np.random.uniform(0, max_disp, (40000, 28, 28, 1))\n",
    "    dataset = (dataset + noise) / (1 + max_disp)\n",
    "    return dataset\n",
    "\n",
    "#function to show images\n",
    "def show_images(examples, n):\n",
    "    for i in range(n * n):\n",
    "        plt.subplot(n, n, 1 + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(examples[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gedk0Gv278nY"
   },
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxCtPy9fkjvH"
   },
   "outputs": [],
   "source": [
    "# load x_train mnist\n",
    "(x_train, _), (_, _) = mnist.load_data()\n",
    "x_train = x_train[:40000] # I left 40000 because this is the maximum dataset size that free google colab can work with\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_train = x_train.reshape((1, 40000, 28, 28, 1))\n",
    "datasets = x_train.copy()\n",
    "\n",
    "max_disp = 0.3\n",
    "steps = 12\n",
    "\n",
    "# create dataset \"datasets\" from noisy images\n",
    "for i in range(steps):\n",
    "    per = add_noise(datasets[i], max_disp).reshape((1, 40000, 28, 28, 1))\n",
    "    datasets = np.concatenate((datasets, per), axis=0)\n",
    "\n",
    "# look at noisy images from \"datasets\"\n",
    "print(f'len dataset: {len(datasets[0])}')\n",
    "for i in range(steps+1):\n",
    "    print(f'dataset number {i}')\n",
    "    show_images(datasets[i], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dug-dyKnp-47"
   },
   "source": [
    "# Denoiser model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcDxk2_XGImQ"
   },
   "outputs": [],
   "source": [
    "# define denoiser\n",
    "def create_denoiser():\n",
    "    model = Sequential([\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPool2D( (2, 2), padding='same'),\n",
    "        BatchNormalization(trainable=True),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPool2D( (2, 2), padding='same'),\n",
    "        BatchNormalization(trainable=True),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Conv2D(1, (3, 3), padding='same', activation='sigmoid'),\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# create list of denoisers\n",
    "models = [create_denoiser() for _ in range(steps)]\n",
    "models[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kON3Ytw8sKpN"
   },
   "outputs": [],
   "source": [
    "# define connection layers\n",
    "def create_connection_layers():\n",
    "    connection = Sequential([\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPool2D( (2, 2), padding='same'),\n",
    "        BatchNormalization(trainable=True),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Conv2D(1, (3, 3), padding='same', activation='sigmoid')\n",
    "    ])\n",
    "    return connection\n",
    "\n",
    "#create list of connection layers\n",
    "models_con = [create_connection_layers() for _ in range(steps)]\n",
    "models_con[0].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDmuUi-iEe4I"
   },
   "source": [
    "# Train denoisers and gan_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hB77lCCklmIW"
   },
   "outputs": [],
   "source": [
    "# define gan_model\n",
    "gan_model = Sequential([\n",
    "    Input(shape=(28, 28, 1)),\n",
    "])\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "for i in range(steps):\n",
    "    # define data in current epoch\n",
    "    data = datasets[steps - i]\n",
    "    # define target in current epoch\n",
    "    target = datasets[steps - i - 1]\n",
    "    if i % 2 == 1:\n",
    "        # train denoiser number i\n",
    "        print(f'############### train denoiser_{i + 1}/{len(models)} ###############')\n",
    "        models[i].fit(data, target, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "        #train layers which will connect 2 denoiser models\n",
    "        print(f'############### train model to connect denoiser_{i + 1} and denoiser_{i} ###############')\n",
    "\n",
    "        # define variable gan\n",
    "        gan_model_per = Sequential([\n",
    "            Input(shape=(28, 28, 1)),\n",
    "        ])\n",
    "\n",
    "        # construct gan\n",
    "        models[i - 1].trainable = False\n",
    "        gan_model_per.add(models[i - 1])\n",
    "        gan_model_per.add(models_con[i - 1])\n",
    "        models[i].trainable = False\n",
    "        gan_model_per.add(models[i])\n",
    "\n",
    "        # compile and train variable gan\n",
    "        gan_model_per.compile(loss='mse', optimizer='adam')\n",
    "        gan_model_per.fit(datasets[steps - i], datasets[steps - i + 1], epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "        # add denoiser his pretrained connection layer\n",
    "        gan_model.add(models[i - 1])\n",
    "        gan_model.add(models_con[i - 1])\n",
    "        gan_model.save_weights('drive/MyDrive/gan_denoiser_1.h5')\n",
    "\n",
    "        # look at losses\n",
    "        print(f'gan_model summary: {gan_model.summary()} /ngan_model_per summary: {gan_model_per.summary()}')\n",
    "    # condition for the first epoch\n",
    "    elif i % 2 == 0:\n",
    "        print(f'############### train denoiser_{i + 1}/{len(models)} ###############')\n",
    "\n",
    "        # train first denoiser\n",
    "        models[i].fit(data, target, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# compile\n",
    "gan_model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "gan_model.save_weights('drive/MyDrive/gan_denoiser_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YSrfIiVRRDq"
   },
   "outputs": [],
   "source": [
    "# make denoisers non-trainable\n",
    "for i in range(steps):\n",
    "    models[i].trainable = True\n",
    "# train conncetion layers\n",
    "print('############### train connection layers ###############')\n",
    "gan_model.fit(datasets[-1], datasets[0], epochs=(epochs * 2), batch_size=batch_size)\n",
    "\n",
    "gan_model.save_weights('drive/MyDrive/gan_denoiser_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70wI_Q3mR_JE"
   },
   "outputs": [],
   "source": [
    "# final training with\n",
    "print('############### train full gan_model ###############')\n",
    "\n",
    "# make denoisers trainable\n",
    "for i in range(steps):\n",
    "    models[i].trainable = True\n",
    "# train full gan_model\n",
    "#gan_model.fit(datasets[-1], datasets[0], epochs=epochs // 2, batch_size=batch_size)\n",
    "\n",
    "gan_model.save_weights('drive/MyDrive/gan_denoiser_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en1xwBa9ISwW"
   },
   "source": [
    "# Look at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlfbBZj4tHVz"
   },
   "outputs": [],
   "source": [
    "b = np.random.randn(121 * 28 * 28 * 1)\n",
    "b = b.reshape(121, 28, 28, 1)\n",
    "pred = gan_model.predict(b)\n",
    "print('noise prediction')\n",
    "show_images(pred, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiHpWRI7JdUb"
   },
   "source": [
    "## Saving model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfbOfttzJltT"
   },
   "outputs": [],
   "source": [
    "gan_model.save_weights('./denoiser_gan_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kINv3GCzIYbP"
   },
   "source": [
    "# Weights loading and fast model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uD67iRbbIsbd"
   },
   "source": [
    "## Fast model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3TzAl1LOI1X6"
   },
   "outputs": [],
   "source": [
    "def create_denoiser():\n",
    "    model = Sequential([\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPool2D( (2, 2), padding='same'),\n",
    "        BatchNormalization(trainable=True),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPool2D( (2, 2), padding='same'),\n",
    "        BatchNormalization(trainable=True),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Conv2D(1, (3, 3), padding='same', activation='sigmoid'),\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "models = [create_denoiser() for _ in range(steps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UL-P2nLGI8Cy"
   },
   "outputs": [],
   "source": [
    "def create_connection_layers():\n",
    "    connection = Sequential([\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPool2D( (2, 2), padding='same'),\n",
    "        BatchNormalization(trainable=True),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Conv2D(1, (3, 3), padding='same', activation='sigmoid')\n",
    "    ])\n",
    "    return connection\n",
    "\n",
    "models_con = [create_connection_layers() for _ in range(steps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7Aouc8SHjBL"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMp1U9IbiiXC"
   },
   "outputs": [],
   "source": [
    "for i in range(steps):\n",
    "    models[i].trainable = False\n",
    "\n",
    "gan_model = Sequential([Input(shape=(28, 28, 1)),])\n",
    "\n",
    "for i in range(0, steps, 2):\n",
    "    gan_model.add(models[i])\n",
    "    gan_model.add(models_con[i])\n",
    "\n",
    "gan_model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p5DxpE_Ixtf"
   },
   "source": [
    "## Loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rc7cDYVctjAw"
   },
   "outputs": [],
   "source": [
    "#gan_model.save_weights('./denoiser_gan_weights.h5')\n",
    "gan_model.load_weights('drive/MyDrive/gan_denoiser_1.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "HoAR95PcLgce",
    "0bEjm4Y9LlAp",
    "j4rJVwtu7zXG",
    "gedk0Gv278nY",
    "Dug-dyKnp-47",
    "LiHpWRI7JdUb",
    "uD67iRbbIsbd"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
